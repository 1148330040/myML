# coding:utf-8
from __future__ import division

import numpy as np
from matplotlib import pyplot as plt
import pandas as pd
import re

pd.set_option('display.max_columns', None)
import warnings
warnings.filterwarnings(action = 'ignore') #å¿½ç•¥è­¦å‘Š

import seaborn as sns
import scipy.stats as sts
from sklearn.metrics import classification_report,f1_score
from sklearn.metrics import confusion_matrix



path = "kaggle\\COTR\\"
datas = pd.read_csv(path+'train.csv', iterator=True)
tests =  pd.read_csv(path+'test.csv', iterator=True)
trains = datas.get_chunk(30000)
#trains.dropna(inplace=True)
tests = tests.get_chunk(5000)
features = trains.columns[3:]
features_int = ['identity_annotator_count', 'toxicity_annotator_count', 'disagree', 'likes', 'sad',
                'wow', 'funny', 'article_id', 'publication_id']
feature_object = ['rating', 'created_date']
for f_i in features_int:
    if f_i != 'article_id':
        trains[f_i] = trains[f_i].astype(np.int16)
for f_f in features:
    if f_f not in features_int and f_f not in feature_object:
        trains[f_f] = trains[f_f].astype(np.float)

# æ ¹æ®ç‰¹å¾å±žæ€§åˆ’åˆ†ä¸åŒçš„ç‰¹å¾ç»„
features_religion = ['hindu', 'buddhist', 'christian',
                     'muslim', 'atheist', 'other_religion'] # å®—æ•™ç‰¹å¾
features_ethnicity = ['white', 'black', 'asian', 'latino',
                      'jewish', 'other_race_or_ethnicity'] # ç§æ—ç‰¹å¾
features_sexual = ['heterosexual', 'homosexual_gay_or_lesbian',
                   'other_sexual_orientation', 'bisexual'] #  å–å‘ç‰¹å¾
features_gender = ['male', 'female', 'transgender', 'other_gender'] # æ€§åˆ«ç‰¹å¾
features_disability = ['intellectual_or_learning_disability', 'psychiatric_or_mental_illness',
                       'physical_disability', 'other_disability'] # å¥åº·ç–¾ç—…ç‰¹å¾
X = ['asian', 'atheist',
       'bisexual', 'black', 'buddhist', 'christian', 'female',
       'heterosexual', 'hindu', 'homosexual_gay_or_lesbian',
       'intellectual_or_learning_disability', 'jewish', 'latino', 'male',
       'muslim', 'other_disability', 'other_gender',
       'other_race_or_ethnicity', 'other_religion',
       'other_sexual_orientation', 'physical_disability',
       'psychiatric_or_mental_illness', 'transgender', 'white']
features_comment = ['rating', 'likes', 'sad', 'wow', 'funny', 'disagree'] # å…³äºŽè¯„è®ºæœ¬èº«çš„å±žæ€§å’Œç‚¹èµžäººæ•°
features_ids = ['article_id', 'parent_id', 'publication_id',
                'identity_annotator_count', 'toxicity_annotator_count'] # å…³äºŽå—è¯„è®ºçš„å†…å®¹æœ¬èº«
times = ['created_date'] # å¯¹äºŽtestæ•°æ®åŒæ ·æ— ç”¨

# ç®€å•æ¥è¯´è¿™ä¸¤éƒ¨åˆ†ç‰¹å¾å¯¹äºŽtestæ•°æ®éƒ½æ˜¯æ²¡æœ‰ä»·å€¼å› ä¸ºå…¶äº§ç”Ÿå¹¶ä¸ä¼šä½“çŽ°åœ¨è¯„è®ºæœ¬èº«é‡Œé¢è€Œæ˜¯å—å¤–éƒ¨å› ç´ å½±å“
features_s = features_disability + features_gender + \
             features_sexual + features_ethnicity + features_religion
print(len(X), len(features_s))

others_features = ['severe_toxicity', 'obscene', 'identity_attack', 'insult',
                   'threat', 'sexual_explicit']
def perform_preprocessing(train, test):
    contract = {
        "'SB91':'senate bill','tRump':'trump','utmterm':'utm term','FakeNews':'fake news','GÊ€á´‡at':'great','Ê™á´á´›toá´':'bottom','washingtontimes':'washington times','garycrum':'gary crum','htmlutmterm':'html utm term','RangerMC':'car','TFWs':'tuition fee waiver','SJWs':'social justice warrior','Koncerned':'concerned','Vinis':'vinys','Yá´á´œ':'you','Trumpsters':'trump','Trumpian':'trump','bigly':'big league','Trumpism':'trump','Yoyou':'you','Auwe':'wonder','Drumpf':'trump','utmterm':'utm term','Brexit':'british exit','utilitas':'utilities','á´€':'a', 'ðŸ˜‰':'wink','ðŸ˜‚':'joy','ðŸ˜€':'stuck out tongue', 'theguardian':'the guardian','deplorables':'deplorable', 'theglobeandmail':'the globe and mail', 'justiciaries': 'justiciary','creditdation': 'Accreditation','doctrne':'doctrine','fentayal': 'fentanyl','designation-': 'designation','CONartist' : 'con-artist','Mutilitated' : 'Mutilated','Obumblers': 'bumblers','negotiatiations': 'negotiations','dood-': 'dood','irakis' : 'iraki','cooerate': 'cooperate','COx':'cox','racistcomments':'racist comments','envirnmetalists': 'environmentalists',Trump's": 'trump is',
        "'cause": 'because', ',cause': 'because', ';cause': 'because', "ain't": 'am not', 'ain,t': 'am not',
        'ain;t': 'am not', 'ainÂ´t': 'am not', 'ainâ€™t': 'am not', "aren't": 'are not',
        'aren,t': 'are not', 'aren;t': 'are not', 'arenÂ´t': 'are not', 'arenâ€™t': 'are not', "can't": 'cannot',
        "can't've": 'cannot have', 'can,t': 'cannot', 'can,t,ve': 'cannot have',
        'can;t': 'cannot', 'can;t;ve': 'cannot have', 'canÂ´t': 'cannot', 'canÂ´tÂ´ve': 'cannot have', 'canâ€™t': 'cannot',
        'canâ€™tâ€™ve': 'cannot have',
        "could've": 'could have', 'could,ve': 'could have', 'could;ve': 'could have', "couldn't": 'could not',
        "couldn't've": 'could not have', 'couldn,t': 'could not', 'couldn,t,ve': 'could not have',
        'couldn;t': 'could not',
        'couldn;t;ve': 'could not have', 'couldnÂ´t': 'could not',
        'couldnÂ´tÂ´ve': 'could not have', 'couldnâ€™t': 'could not', 'couldnâ€™tâ€™ve': 'could not have',
        'couldÂ´ve': 'could have',
        'couldâ€™ve': 'could have', "didn't": 'did not', 'didn,t': 'did not', 'didn;t': 'did not', 'didnÂ´t': 'did not',
        'didnâ€™t': 'did not', "doesn't": 'does not', 'doesn,t': 'does not', 'doesn;t': 'does not', 'doesnÂ´t': 'does not',
        'doesnâ€™t': 'does not', "don't": 'do not', 'don,t': 'do not', 'don;t': 'do not', 'donÂ´t': 'do not',
        'donâ€™t': 'do not',
        "hadn't": 'had not', "hadn't've": 'had not have', 'hadn,t': 'had not', 'hadn,t,ve': 'had not have',
        'hadn;t': 'had not',
        'hadn;t;ve': 'had not have', 'hadnÂ´t': 'had not', 'hadnÂ´tÂ´ve': 'had not have', 'hadnâ€™t': 'had not',
        'hadnâ€™tâ€™ve': 'had not have', "hasn't": 'has not', 'hasn,t': 'has not', 'hasn;t': 'has not', 'hasnÂ´t': 'has not',
        'hasnâ€™t': 'has not',
        "haven't": 'have not', 'haven,t': 'have not', 'haven;t': 'have not', 'havenÂ´t': 'have not',
        'havenâ€™t': 'have not', "he'd": 'he would',
        "he'd've": 'he would have', "he'll": 'he will',
        "he's": 'he is', 'he,d': 'he would', 'he,d,ve': 'he would have', 'he,ll': 'he will', 'he,s': 'he is',
        'he;d': 'he would',
        'he;d;ve': 'he would have', 'he;ll': 'he will', 'he;s': 'he is', 'heÂ´d': 'he would', 'heÂ´dÂ´ve': 'he would have',
        'heÂ´ll': 'he will',
        'heÂ´s': 'he is', 'heâ€™d': 'he would', 'heâ€™dâ€™ve': 'he would have', 'heâ€™ll': 'he will', 'heâ€™s': 'he is',
        "how'd": 'how did', "how'll": 'how will',
        "how's": 'how is', 'how,d': 'how did', 'how,ll': 'how will', 'how,s': 'how is', 'how;d': 'how did',
        'how;ll': 'how will',
        'how;s': 'how is', 'howÂ´d': 'how did', 'howÂ´ll': 'how will', 'howÂ´s': 'how is', 'howâ€™d': 'how did',
        'howâ€™ll': 'how will',
        'howâ€™s': 'how is', "i'd": 'i would', "i'll": 'i will', "i'm": 'i am', "i've": 'i have', 'i,d': 'i would',
        'i,ll': 'i will',
        'i,m': 'i am', 'i,ve': 'i have', 'i;d': 'i would', 'i;ll': 'i will', 'i;m': 'i am', 'i;ve': 'i have',
        "isn't": 'is not',
        'isn,t': 'is not', 'isn;t': 'is not', 'isnÂ´t': 'is not', 'isnâ€™t': 'is not', "it'd": 'it would',
        "it'll": 'it will', "It's": 'it is',
        "it's": 'it is', 'it,d': 'it would', 'it,ll': 'it will', 'it,s': 'it is', 'it;d': 'it would',
        'it;ll': 'it will', 'it;s': 'it is', 'itÂ´d': 'it would', 'itÂ´ll': 'it will', 'itÂ´s': 'it is',
        'itâ€™d': 'it would', 'itâ€™ll': 'it will', 'itâ€™s': 'it is',
        'iÂ´d': 'i would', 'iÂ´ll': 'i will', 'iÂ´m': 'i am', 'iÂ´ve': 'i have', 'iâ€™d': 'i would', 'iâ€™ll': 'i will',
        'iâ€™m': 'i am',
        'iâ€™ve': 'i have', "let's": 'let us', 'let,s': 'let us', 'let;s': 'let us', 'letÂ´s': 'let us',
        'letâ€™s': 'let us', "ma'am": 'madam', 'ma,am': 'madam', 'ma;am': 'madam', "mayn't": 'may not',
        'mayn,t': 'may not', 'mayn;t': 'may not',
        'maynÂ´t': 'may not', 'maynâ€™t': 'may not', 'maÂ´am': 'madam', 'maâ€™am': 'madam', "might've": 'might have',
        'might,ve': 'might have', 'might;ve': 'might have', "mightn't": 'might not', 'mightn,t': 'might not',
        'mightn;t': 'might not', 'mightnÂ´t': 'might not',
        'mightnâ€™t': 'might not', 'mightÂ´ve': 'might have', 'mightâ€™ve': 'might have', "must've": 'must have',
        'must,ve': 'must have', 'must;ve': 'must have',
        "mustn't": 'must not', 'mustn,t': 'must not', 'mustn;t': 'must not', 'mustnÂ´t': 'must not',
        'mustnâ€™t': 'must not', 'mustÂ´ve': 'must have',
        'mustâ€™ve': 'must have', "needn't": 'need not', 'needn,t': 'need not', 'needn;t': 'need not',
        'neednÂ´t': 'need not', 'neednâ€™t': 'need not', "oughtn't": 'ought not', 'oughtn,t': 'ought not',
        'oughtn;t': 'ought not',
        'oughtnÂ´t': 'ought not', 'oughtnâ€™t': 'ought not', "sha'n't": 'shall not', 'sha,n,t': 'shall not',
        'sha;n;t': 'shall not', "shan't": 'shall not',
        'shan,t': 'shall not', 'shan;t': 'shall not', 'shanÂ´t': 'shall not', 'shanâ€™t': 'shall not',
        'shaÂ´nÂ´t': 'shall not', 'shaâ€™nâ€™t': 'shall not',
        "she'd": 'she would', "she'll": 'she will', "she's": 'she is', 'she,d': 'she would', 'she,ll': 'she will',
        'she,s': 'she is', 'she;d': 'she would', 'she;ll': 'she will', 'she;s': 'she is', 'sheÂ´d': 'she would',
        'sheÂ´ll': 'she will',
        'sheÂ´s': 'she is', 'sheâ€™d': 'she would', 'sheâ€™ll': 'she will', 'sheâ€™s': 'she is', "should've": 'should have',
        'should,ve': 'should have', 'should;ve': 'should have',
        "shouldn't": 'should not', 'shouldn,t': 'should not', 'shouldn;t': 'should not', 'shouldnÂ´t': 'should not',
        'shouldnâ€™t': 'should not', 'shouldÂ´ve': 'should have',
        'shouldâ€™ve': 'should have', "that'd": 'that would', "that's": 'that is', 'that,d': 'that would',
        'that,s': 'that is', 'that;d': 'that would',
        'that;s': 'that is', 'thatÂ´d': 'that would', 'thatÂ´s': 'that is', 'thatâ€™d': 'that would', 'thatâ€™s': 'that is',
        "there'd": 'there had',
        "there's": 'there is', 'there,d': 'there had', 'there,s': 'there is', 'there;d': 'there had',
        'there;s': 'there is',
        'thereÂ´d': 'there had', 'thereÂ´s': 'there is', 'thereâ€™d': 'there had', 'thereâ€™s': 'there is',
        "they'd": 'they would', "they'll": 'they will', "they're": 'they are', "they've": 'they have',
        'they,d': 'they would', 'they,ll': 'they will', 'they,re': 'they are', 'they,ve': 'they have',
        'they;d': 'they would', 'they;ll': 'they will', 'they;re': 'they are',
        'they;ve': 'they have', 'theyÂ´d': 'they would', 'theyÂ´ll': 'they will', 'theyÂ´re': 'they are',
        'theyÂ´ve': 'they have', 'theyâ€™d': 'they would', 'theyâ€™ll': 'they will',
        'theyâ€™re': 'they are', 'theyâ€™ve': 'they have', "wasn't": 'was not', 'wasn,t': 'was not', 'wasn;t': 'was not',
        'wasnÂ´t': 'was not',
        'wasnâ€™t': 'was not', "we'd": 'we would', "we'll": 'we will', "we're": 'we are', "we've": 'we have',
        'we,d': 'we would', 'we,ll': 'we will',
        'we,re': 'we are', 'we,ve': 'we have', 'we;d': 'we would', 'we;ll': 'we will', 'we;re': 'we are',
        'we;ve': 'we have',
        "weren't": 'were not', 'weren,t': 'were not', 'weren;t': 'were not', 'werenÂ´t': 'were not',
        'werenâ€™t': 'were not', 'weÂ´d': 'we would', 'weÂ´ll': 'we will',
        'weÂ´re': 'we are', 'weÂ´ve': 'we have', 'weâ€™d': 'we would', 'weâ€™ll': 'we will', 'weâ€™re': 'we are',
        'weâ€™ve': 'we have', "what'll": 'what will', "what're": 'what are', "what's": 'what is',
        "what've": 'what have', 'what,ll': 'what will', 'what,re': 'what are', 'what,s': 'what is',
        'what,ve': 'what have', 'what;ll': 'what will', 'what;re': 'what are',
        'what;s': 'what is', 'what;ve': 'what have', 'whatÂ´ll': 'what will',
        'whatÂ´re': 'what are', 'whatÂ´s': 'what is', 'whatÂ´ve': 'what have', 'whatâ€™ll': 'what will',
        'whatâ€™re': 'what are', 'whatâ€™s': 'what is',
        'whatâ€™ve': 'what have', "where'd": 'where did', "where's": 'where is', 'where,d': 'where did',
        'where,s': 'where is', 'where;d': 'where did',
        'where;s': 'where is', 'whereÂ´d': 'where did', 'whereÂ´s': 'where is', 'whereâ€™d': 'where did',
        'whereâ€™s': 'where is',
        "who'll": 'who will', "who's": 'who is', 'who,ll': 'who will', 'who,s': 'who is', 'who;ll': 'who will',
        'who;s': 'who is',
        'whoÂ´ll': 'who will', 'whoÂ´s': 'who is', 'whoâ€™ll': 'who will', 'whoâ€™s': 'who is', "won't": 'will not',
        'won,t': 'will not', 'won;t': 'will not',
        'wonÂ´t': 'will not', 'wonâ€™t': 'will not', "wouldn't": 'would not', 'wouldn,t': 'would not',
        'wouldn;t': 'would not', 'wouldnÂ´t': 'would not',
        'wouldnâ€™t': 'would not', "you'd": 'you would', "you'll": 'you will', "you're": 'you are', 'you,d': 'you would',
        'you,ll': 'you will',
        'you,re': 'you are', 'you;d': 'you would', 'you;ll': 'you will',
        'you;re': 'you are', 'youÂ´d': 'you would', 'youÂ´ll': 'you will', 'youÂ´re': 'you are', 'youâ€™d': 'you would',
        'youâ€™ll': 'you will', 'youâ€™re': 'you are',
        'Â´cause': 'because', 'â€™cause': 'because', "you've": "you have", "could'nt": 'could not',
        "havn't": 'have not', "hereâ€™s": "here is", 'i""m': 'i am', "i'am": 'i am', "i'l": "i will", "i'v": 'i have',
        "wan't": 'want', "was'nt": "was not", "who'd": "who would",
        "who're": "who are", "who've": "who have", "why'd": "why would", "would've": "would have", "y'all": "you all",
        "y'know": "you know", "you.i": "you i",
        "your'e": "you are", "arn't": "are not", "agains't": "against", "c'mon": "common", "doens't": "does not",
        'don""t': "do not", "dosen't": "does not",
        "dosn't": "does not", "shoudn't": "should not", "that'll": "that will", "there'll": "there will",
        "there're": "there are",
        "this'll": "this all", "u're": "you are", "ya'll": "you all", "you'r": "you are", "youâ€™ve": "you have",
        "d'int": "did not", "did'nt": "did not", "din't": "did not", "dont't": "do not", "gov't": "government",
        "i'ma": "i am", "is'nt": "is not", "â€˜I": 'I',
        'á´€É´á´…': 'and', 'á´›Êœá´‡': 'the', 'Êœá´á´á´‡': 'home', 'á´œá´˜': 'up', 'Ê™Ê': 'by', 'á´€á´›': 'at', 'â€¦and': 'and',
        'civilbeat': 'civil beat', \
        'TrumpCare': 'Trump care', 'Trumpcare': 'Trump care', 'OBAMAcare': 'Obama care', 'á´„Êœá´‡á´„á´‹': 'check', 'Ò“á´Ê€': 'for',
        'á´›ÊœÉªs': 'this', 'á´„á´á´á´˜á´œá´›á´‡Ê€': 'computer', \
        'á´á´É´á´›Êœ': 'month', 'á´¡á´Ê€á´‹ÉªÉ´É¢': 'working', 'á´Šá´Ê™': 'job', 'Ò“Ê€á´á´': 'from', 'Sá´›á´€Ê€á´›': 'start', 'gubmit': 'submit',
        'COâ‚‚': 'carbon dioxide', 'Ò“ÉªÊ€sá´›': 'first', \
        'á´‡É´á´…': 'end', 'á´„á´€É´': 'can', 'Êœá´€á´ á´‡': 'have', 'á´›á´': 'to', 'ÊŸÉªÉ´á´‹': 'link', 'á´Ò“': 'of', 'Êœá´á´œÊ€ÊŸÊ': 'hourly',
        'á´¡á´‡á´‡á´‹': 'week', 'á´‡É´á´…': 'end', 'á´‡xá´›Ê€á´€': 'extra', \
        'GÊ€á´‡á´€á´›': 'great', 'sá´›á´œá´…á´‡É´á´›s': 'student', 'sá´›á´€Ê': 'stay', 'á´á´á´s': 'mother', 'á´Ê€': 'or', 'á´€É´Êá´É´á´‡': 'anyone',
        'É´á´‡á´‡á´…ÉªÉ´É¢': 'needing', 'á´€É´': 'an', 'ÉªÉ´á´„á´á´á´‡': 'income', \
        'Ê€á´‡ÊŸÉªá´€Ê™ÊŸá´‡': 'reliable', 'Ò“ÉªÊ€sá´›': 'first', 'Êá´á´œÊ€': 'your', 'sÉªÉ¢É´ÉªÉ´É¢': 'signing', 'Ê™á´á´›á´›á´á´': 'bottom',
        'Ò“á´ÊŸÊŸá´á´¡ÉªÉ´É¢': 'following', 'Má´€á´‹á´‡': 'make', \
        'á´„á´É´É´á´‡á´„á´›Éªá´É´': 'connection', 'ÉªÉ´á´›á´‡Ê€É´á´‡á´›': 'internet', 'financialpost': 'financial post', 'Êœaá´ á´‡': ' have ',
        'á´„aÉ´': ' can ', 'Maá´‹á´‡': ' make ', 'Ê€á´‡ÊŸÉªaÊ™ÊŸá´‡': ' reliable ', 'É´á´‡á´‡á´…': ' need ',
        'á´É´ÊŸÊ': ' only ', 'á´‡xá´›Ê€a': ' extra ', 'aÉ´': ' an ', 'aÉ´Êá´É´á´‡': ' anyone ', 'sá´›aÊ': ' stay ', 'Sá´›aÊ€á´›': ' start',
        'SHOPO': 'shop',
        }
    punct = [',', '.', '"', ':', 'ðŸ¢', ')', 'ðŸ´', 'ðŸµ', '(', '-', '!', '?', '|', ';', "'", '$', '&', '/', '[', ']',
          '>', '%', '=', '#', '*', '+', '\\', 'â€¢', '~', '@', 'Â£', 'Â·', '_', '{', '}', 'Â©', '^','\n'
          'Â®', '`', '<', 'â†’', 'Â°', 'â‚¬', 'â„¢', 'â€º', 'â™¥', 'â†', 'Ã—', 'Â§', 'â€³', 'â€²', 'Ã‚', 'â–ˆ',
          'Â½', 'Ã ', 'â€¦', 'â€œ', 'â˜…', 'â€', 'â€“', 'â—', 'Ã¢', 'â–º', 'âˆ’', 'Â¢', 'Â²', 'Â¬', 'â–‘', 'Â¶',
          'â†‘', 'Â±', 'Â¿', 'â–¾', 'â•', 'Â¦', 'â•‘', 'â€•', 'Â¥', 'â–“', 'â€”', 'â€¹', 'â”€', 'â–’', 'ï¼š', 'Â¼',
          'âŠ•', 'â–¼', 'â–ª', 'â€ ', 'â– ', 'â€™', 'â–€', 'Â¨', 'â–„', 'â™«', 'â˜†', 'Ã©', 'Â¯', 'â™¦', 'Â¤', 'â–²',
          'Ã¨', 'Â¸', 'Â¾', 'Ãƒ', 'â‹…', 'â€˜', 'âˆž', 'âˆ™', 'ï¼‰', 'â†“', 'ã€', 'â”‚', 'ï¼ˆ', 'Â»', 'ï¼Œ', 'â™ª',
          'â•©', 'â•š', 'Â³', 'ãƒ»', 'â•¦', 'â•£', 'â•”', 'â•—', 'â–¬', 'â¤', 'Ã¯', 'Ã˜', 'Â¹', 'â‰¤', 'â€¡', 'âˆš']
    def clean_special_chars(text, punct, mapping):
        for p in mapping:
            text = text.replace(p, mapping[p].lower())
        for p in punct:
            text = text.replace(p, ' ')
        return text

    for df in [train, test]:
        df['comment_text'] = df['comment_text'].astype(str)
        df['comment_text'] = df['comment_text'].apply(lambda x: clean_special_chars(x, punct, contract))

    return train, test

trains, tests = perform_preprocessing(trains[['id', 'target', 'comment_text']], tests)
def get_new_features(datas):
    features_religion = ['hindu', 'buddhist', 'christian',
                         'muslim', 'atheist', 'other_religion']  # å®—æ•™ç‰¹å¾
    features_ethnicity = ['white', 'black', 'asian', 'latino',
                          'jewish', 'other_race_or_ethnicity']  # ç§æ—ç‰¹å¾
    features_sexual = ['heterosexual', 'homosexual_gay_or_lesbian',
                       'other_sexual_orientation', 'bisexual']  # å–å‘ç‰¹å¾
    features_gender = ['male', 'female', 'transgender', 'other_gender']  # æ€§åˆ«ç‰¹å¾
    features_disability = ['intellectual_or_learning_disability', 'psychiatric_or_mental_illness',
                           'physical_disability', 'other_disability']  # å¥åº·ç–¾ç—…ç‰¹å¾
    pattern = r'\s'
    datas['comment_text'] = datas['comment_text'].apply(
        lambda x: list(re.split(pattern, x))
    ).astype(str)
    for i in features_religion:
        datas[i+'_count_text'] = datas['comment_text'].apply(lambda x: x.lower().count(i))
    ethnicity_word = ['latinos', 'hispanic', 'hispanics', 'negger', 'negro', 'mongoloid', 'caucasian', 'jewess', 'jew']
    features_ethnicity = features_ethnicity + ethnicity_word
    for i in features_ethnicity:
        datas[i+'_count_text'] = datas['comment_text'].apply(lambda x: x.lower().count(i))
    new_features_sexual = ['gay', 'bisexual', 'heterosexual',
                           'homosexual', 'lesbian']
    # å…³äºŽè¿™ä¸€éƒ¨åˆ†çš„è¨€è®ºï¼Œè¯´å®žè¯å¾ˆéš¾åŽ»å®šä¹‰æ˜¯å¦æœ‰æ¶ï¼Œä¹‹åŽå¯èƒ½éœ€è¦å®šä¹‰æƒé‡ç»™è¿™äº›ç‰¹å¾
    for i in new_features_sexual:
        datas[i+'_count_text'] = datas['comment_text'].apply(lambda x: x.lower().count(i))
    new_features_gender_word = ['sexism', 'ageism', 'sexist']
    features_gender = features_gender + new_features_gender_word
    for i in features_gender:
        datas[i+'_count_text'] = datas['comment_text'].apply(lambda x: x.lower().count(i))
    # åŒæ ·å…³äºŽè¿™éƒ¨åˆ†çš„ç‰¹å¾é™¤äº†æ˜Žç¡®çš„å•è¯å¤–ç±»ä¼¼äºŽmale,fmealéƒ½æ˜¯ååˆ†ä¸­æ€§çš„è¯ï¼ŒåŒæ ·éœ€è¦ç‰¹æ®Šå¤„ç†
    new_features_disability = ['mentally', 'ill', 'physical',
                               'disability', 'mental', 'disable',
                               'psychiatric']
    for i in new_features_disability:
        datas[i+'_count_text'] = datas['comment_text'].apply(lambda x: x.lower().count(i))
    datas['new_disability_count_text'] = 0
    for i in new_features_disability:
        datas['new_disability_count_text'] += datas[i+'_count_text']
        datas.drop(i+'_count_text', axis=1, inplace=True)
    datas['new_disability_count_text'] = datas['new_disability_count_text'].apply(lambda x: 1 if x>3 else 0)
    datas.drop('comment_text', axis=1, inplace=True)
    datas['special_value_sum'] = datas.apply(lambda x: x.sum(), axis=1)
    datas['special_value_std'] = datas.apply(lambda x: x.std(), axis=1)
    datas['special_value_mean'] = datas['special_value_sum'] / len(datas.columns)
    return datas

other_trains = get_new_features(trains[['comment_text']])
other_tests = get_new_features(tests[['comment_text']])



# ä½¿ç”¨tf-Idfå¤„ç†å¤±è´¥
"""
trains_poison = trains.loc[trains.target > 0.5]
trains_nopoison = trains.loc[trains.target == 0]
others_features_ids = []
others_features_ids_n = []
for i in others_features:
    others_features_ids = others_features_ids + list(trains_poison.loc[trains_poison[i] > 0.5].id.values)
    others_features_ids_n = others_features_ids_n + list(trains_nopoison[trains_nopoison[i]==0].id.values)

others_features_comments = []
X = ['0 ', 'z']
for i in set(others_features_ids):
    #others_features_comments.append(trains_poison.loc[trains_poison.id==i].comment_text.values[0])
    X[0] = X[0] + trains_poison.loc[trains_poison.id==i].comment_text.values[0]
others_features_comments_n = []
print("zxcasd")
print(X)
Y = ['0', 'z']
import random
for i in set(random.sample(others_features_ids_n, 100)):
    others_features_comments_n.append(trains_nopoison.loc[trains_nopoison.id==i].comment_text.values[0])
    Y[0] = Y[0] + trains_nopoison.loc[trains_nopoison.id==i].comment_text.values[0]
print(Y)
from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer
tf = TfidfTransformer()
ct = CountVectorizer()
others_features_comments = ct.fit_transform(X)
others_features_comments_tfidf = tf.fit_transform(others_features_comments)
words = ct.get_feature_names()  # æ‰€æœ‰æ–‡æœ¬çš„å…³é”®å­—
weight = others_features_comments_tfidf.toarray()
new_others_features_words = []
sort_weight = np.argsort(weight[0])
for i in range(100):
    new_others_features_words.append(words[sort_weight[i]])
print(set(new_others_features_words))

'''
for w in weight:
    loc = np.argsort(-w)
    for i in range(1):
        new_others_features_words.append(words[loc[i]])
new_others_features_words = set(new_others_features_words)
'''

#others_features_comments_n = random.sample(others_features_comments_n, 100)
others_features_comments_n = ct.fit_transform(Y)
others_features_comments_tfidf_n = tf.fit_transform(others_features_comments_n)
words_n = ct.get_feature_names()  # æ‰€æœ‰æ–‡æœ¬çš„å…³é”®å­—
weight_n = others_features_comments_tfidf.toarray()
new_others_features_words_n = []
sort_weight_n = np.argsort(-weight_n[0])
for i in range(100):
    new_others_features_words_n.append(words_n[sort_weight_n[i]])
print(new_others_features_words_n)
'''
for w in weight_n:
    loc = np.argsort(-w)
    for i in range(1):
        new_others_features_words_n.append(words_n[loc[i]])
new_others_features_words_n = set(new_others_features_words_n)
'''
remove_words = []
for i in new_others_features_words:
    if i in new_others_features_words_n:
        remove_words.append(i)
for i in remove_words:
    new_others_features_words.remove(i)
print(len(new_others_features_words))
print(new_others_features_words)
"""
trains['target'] = trains['target'].apply(lambda x: 1 if x>0.5 else 0)
train = trains[:25000]
label_train = train.pop('target')
other_trains_1 = other_trains[:25000]
tests = trains[25000:]
other_trains_2 = other_trains[25000:]
label_test  = tests.pop('target')
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
tokenizer = Tokenizer(num_words=200)
tokenizer.fit_on_texts(list(trains['comment_text'])+list(tests['comment_text']))
word_index = tokenizer.word_index
train_X = tokenizer.texts_to_sequences(train['comment_text'])
test_X = tokenizer.texts_to_sequences(tests['comment_text'])
train_X = pad_sequences(train_X, maxlen=220)
test_X = pad_sequences(test_X, maxlen=220)

train_X = np.hstack([train_X, other_trains_1])
test_X = np.hstack([test_X, other_trains_2])


from sklearn.model_selection import StratifiedKFold

params = {
    'max_depth':-1,
    'n_estimators':1000,
    'learning_rate':0.05,
    'num_leaves':2**9-1,
    'colsample_bytree':0.28,
    'objective':'binary',
    'n_jobs':-1,
    'eval_metric':'auc'
}
import lightgbm as lgb
xtrain = lgb.Dataset(train_X, label_train)
num_round = 10000
lgb = lgb.train(params, xtrain, num_round)
yp = lgb.predict(test_X)
from sklearn.metrics import roc_auc_score,f1_score

print(roc_auc_score(list(label_test.values), list(yp)))

